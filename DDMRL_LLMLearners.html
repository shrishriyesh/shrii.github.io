<!DOCTYPE HTML>
<!--
  Forty by HTML5 UP – heavily pruned & retitled for
  "Verifiable Rewards: Enforcing Correct Reasoning in LLMs via RL" (RLVR)
  All *FIGURE* comments mark slide‑image placeholders for you to swap in.
-->
<html>
  <head>
    <title>Verifiable Rewards (RLVR)</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
  </head>
  <body class="is-preload">

    <!-- Wrapper -->
    <div id="wrapper">

      <!-- Header -->
      <header id="header" class="alt style2">
        <a href="index.html" class="logo"><strong>DDMRL</strong> <span>CSCI-GA 3033-090: Deep Decision Making and Reinforcement Learning
        </span></a>
        <nav>
          <a href="#menu">Menu</a>
        </nav>
      </header>

      <!-- Banner (hero) -->
      <section id="banner" class="style2">
        <div class="inner">
          <span class="image">
            <!-- *FIGURE‑HERO*  – 1920×1080 hero banner from slide 1 -->
            <img src="images/hero_placeholder.jpg" alt="Verifiable Rewards hero" />
          </span>
          <header class="major">
            <h1>Verifiable Rewards</h1>
          </header>
          <div class="content">
            <p>Enforcing Correct Reasoning in LLMs via Reinforcement Learning</p>
          </div>
        </div>
      </section>

      <!-- Main body -->
      <div id="main">

        <!-- 1 · Overview -->
        <section id="overview" class="inner">
          <header class="major">
            <h2>Project Overview</h2>
          </header>
          <p>
            In the Reinforcement Learning with Verifiable Rewards (RLVR) training approach we add an extra reasoning-based reward to the learning process. In this project, a student language model learns to solve problems with feedback not only on whether its final answer is correct, but also on the reasoning it used (when applicable). A larger teacher LLM oversees training: the teacher model provides reasoning-based feedback. This means the student is rewarded for correct answers (and proper answer format), but it also gains additional reward if the reasoning somewhat sound in itsefl. By verifying reasoning in each attempt, our training pipeline encourages the student model to develop correct step-by-step problem-solving skills, not just lucky guesses.
          </p>
        </section>

       <!-- 2 · Methodology (flow diagram) -->
      <section id="method" class="spotlights">
      <section>
        <a href="#" class="image">
          <!-- *FIGURE-PIPELINE*  – flowchart showing base→RLVR→R1-Zero 800×600 -->
          <img src="images/RLVR.png" alt="RLVR training pipeline" data-position="center center" />
        </a>
        <div class="content">
          <div class="inner">
            <header class="major"><h3>Training Pipeline</h3></header>
    
            <!-- original intro paragraph kept verbatim -->
            <p>
              The RLVR training pipeline we proprose updates the student LLM using multiple feedback signals with guidance from a larger model. For each training query (in our case, a math problem), the student model generates a solution with a chain-of-thought. We then evaluate the student’s output on three aspects:
            </p>
    
            <!-- switched from br-separated bullets to a proper unordered list -->
            <ul>
              <li>Accuracy of the answer: Did the student get the correct final answer? This yields a primary reward signal (positive if correct, zero/negative if incorrect).</li>
    
              <li>Format compliance: Does the response follow the required format or instructions (e.g. showing reasoning steps and a neatly formatted answer)? If the output is well-formatted, the student earns additional reward for following instructions properly.</li>
    
              <li>
                Reasoning quality (Our Addition): When the optional reasoning-based reward is enabled, the teacher LLM reviews the student’s chain-of-thought for logical soundness and completeness.  
                &nbsp;&nbsp;&bull;&nbsp;<em>Method 1&nbsp;(computationally cheaper):</em> the teacher inspects reasoning only if the final answer is correct, saving compute and avoiding noisy feedback on wrong answers.  
                &nbsp;&nbsp;&bull;&nbsp;<em>Method 2&nbsp;(computationally more expensive):</em> the teacher inspects reasoning for every sample, regardless of correctness, providing richer but costlier feedback.  
                A solution earns extra reward from this step only when its reasoning is coherent and valid.
              </li>
            </ul>
    
            <!-- closing paragraph kept verbatim -->
            <p>
              These reward components are combined to adjust the student model’s policy via reinforcement learning (using a method like GRPO, a variant of PPO). In simple terms, the student gradually learns to maximize these rewards, which means learning to produce accurate, well-formatted, and (when required) well-reasoned answers. Over many training iterations, this pipeline aligns the student model’s behavior with the desired outcomes and reasoning process, guided by feedback from the teacher model on successful solutions.
            </p>
          </div>
        </div>
      </section>
    </section>
         <!-- Baseline slide -->
      <section id="baseline" class="inner">
        <header class="major">
          <h3>Baseline Setup</h3>
        </header>

        <div class="row gtr-50 gtr-uniform">
          <div class="col-12">
            <span class="image fit">
              <img src="images/Baseline.png" alt="Baseline RL diagram" />
            </span>
          </div>
        </div>

        <div class="row gtr-50 gtr-uniform baseline-rewards">
          <div class="col-6 col-12-small">
            <span class="image fit">
              <img src="images/Format_reward.png" alt="Format reward" />
            </span>
            <h4 class="center">Format Correctness ✓</h4>
          </div>
          <div class="col-6 col-12-small">
            <span class="image fit">
              <img src="images/Accuracy_Reward.png" alt="Accuracy reward" />
            </span>
            <h4 class="center">Answer Accuracy ✓</h4>
          </div>
        </div>

       <p>
        As a baseline, we first train the student model with only accuracy and format-based rewards, without any reasoning feedback. In this setup, the reinforcement learning reward function simply checks if the final answer is correct and if the answer is presented in the proper format. The student gets positive feedback for answering correctly and adhering to the expected answer style or format (and zero or negative feedback otherwise). There is no evaluation of the content of the reasoning steps in this baseline. This approach mirrors the strategy used in prior works like DeepSeek-R1 and DeepSeek-MATH, which relied solely on outcome-based rewards (correct answer and format compliance) to improve the model. The baseline establishes how well the model can do with only those basic signals, serving as a point of comparison for our enhanced method.
       </p>
      </section>


        <!-- 3 · Reward criteria cards -->
        <section id="reward" class="inner">
          <header class="major"><h2>Our Model and Proposed Reward Components</h2></header>
        
          <p>
            Our model extends the baseline by adding a <strong>reasoning-correctness reward</strong> on top of the accuracy and format signals.  
            A teacher LLM inspects the student’s chain&nbsp;of&nbsp;thought and assigns a score using one of two modes:
          </p>
        
          <ul>
            <li><strong>Method&nbsp;1 (Computationally Cheaper):</strong>  
                The teacher is called <em>only when the final answer is correct</em>. This limits compute and avoids noisy feedback on wrong answers.  
                Reward scale&nbsp;&rarr;&nbsp;{ 1&nbsp;fully&nbsp;correct&nbsp;&vert;&nbsp;0.5&nbsp;some&nbsp;valid&nbsp;steps&nbsp;&vert;&nbsp;0&nbsp;mostly&nbsp;wrong }.
            </li>
            <li><strong>Method&nbsp;2 (Computationally Expensive):</strong>  
                The teacher is called on <em>every output</em>, granting partial credit even if the answer is wrong.  
                Reward scale&nbsp;&rarr;&nbsp;{ 1&nbsp;answer &amp; reasoning correct&nbsp;&vert;&nbsp;0.66&nbsp;answer correct but gaps&nbsp;&vert;&nbsp;0.33&nbsp;answer wrong but some valid steps&nbsp;&vert;&nbsp;0 largely wrong }.
            </li>
          </ul>
        
          <p>
            This additional signal helps the student learn more from each example, improving sample efficiency—especially on harder tasks such as challenging math problems. Over time, the model is encouraged to produce not just correct answers, but correct answers backed by sound reasoning.
          </p>
        
          <div class="row gtr-50 gtr-uniform">
            <!-- <div class="col-4"> -->
              <!-- *FIGURE-FORMAT* -->
              <!-- <span class="image fit"><img src="images/Format_reward.png" alt="Format reward" /></span>
              <h4 class="center">Format Correctness ✓</h4>
            </div>
            <div class="col-4"> -->
              <!-- *FIGURE-ACCURACY* -->
              <!-- <span class="image fit"><img src="images/Accuracy_Reward.png" alt="Accuracy reward" /></span>
              <h4 class="center">Answer Accuracy ✓</h4>
            </div> -->
            <div class="col-4">
              <!-- *FIGURE-REASONING* -->
              <span class="image fit"><img src="images/Reasoning_Reward.png" alt="Reasoning reward" /></span>
              <h4 class="center">Reasoning Correctness ✓✓</h4>
            </div>
          </div>
        </section>
        
        <!-- 4 · Experimental details -->
        <section id="experiments" class="spotlights">
          <section>
            <a href="#" class="image">
              <!-- *FIGURE-EXP-DETAILS* – slide with model & dataset bullets 900×600 -->
              <img src="images/exp_details.png" alt="Experimental details" data-position="center center" />
            </a>
            <div class="content"><div class="inner">
              <header class="major"><h3>Experimental Setup</h3></header>
              <p>
              For our experiments, we fine-tuned a medium-sized language model using the RLVR approach and the baseline for comparison. Below are the key training details for reproducibility:
            </p>
            
            <ul>
              <li>Base Model: We started with Gemma-3-4B-IT, a 4-billion-parameter instruction-tuned model. We set up two separate fine-tuning runs: one on the GSM8K math word problem dataset, and another on the MATH competition problem dataset. This allows us to evaluate performance on each dataset independently.</li>
            
              <li>Training Duration: Each model was trained for roughly 1,000 RL optimization steps. This is a relatively short training schedule, leveraging the strong pre-trained foundation of Gemma-3-4B-IT and the focused reward signals to quickly improve reasoning ability.</li>
            
              <li>Batching and Sampling: We used a batch size of 1 (one problem prompt at a time). However, for each prompt, the model generated 6 different solution attempts in parallel. We employed the GRPO (Group Relative Policy Optimization) strategy, which means the rewards of these 6 samples were compared to each other. By ranking the samples from best to worst (according to the reward function) and updating the policy to favor the higher-reward ones, we achieve a stable training signal without needing a separate value function. This multi-sample approach helps the model learn from comparative feedback on each question.</li>
            
              <li>Reward Computation: The reward for each sample was computed based on the criteria described above (accuracy, format, and reasoning for the RLVR model). Only the final answer’s correctness and format mattered for the baseline; the RLVR model also got the extra reasoning score from the teacher when applicable.</li>
            
              <li>Regularization (KL Penalty): To ensure the fine-tuning doesn’t push the model too far from its pre-trained behavior (which could cause instability or loss of general language ability), we added a small Kullback–Leibler (KL) divergence penalty. We set β = 0.001 for this KL regularization term. In practice, this gently constrains the updated policy to stay close to the initial model’s output distribution, preventing the student from going off-track due to the narrow reward signals. This technique is common in RL fine-tuning (analogous to what’s done in RLHF) and helps maintain output quality and diversity.</li>
            </ul>
            
            <p>Dataset Used:</p>
            <ol>
              <li>GSM8K (Grade School Math 8K): is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.</li>
            
              <li>MATH Dataset: The MATH dataset is a collection of mathematics competition problems designed to evaluate mathematical reasoning and problem-solving capabilities in computational systems. Containing 12,500 high school competition-level mathematics problems, this dataset is notable for including detailed step-by-step solutions alongside each problem.</li>
            </ol>
            
            <p>
              In summary, our experimental setup fine-tunes a 4B parameter model on two math-focused benchmarks with a novel reward scheme. We generate multiple solutions per prompt (using GRPO) and apply a slight KL regularization, which together ensure efficient and stable training under the RLVR framework. We trained and evaluates our models separately on two datasets i.e., MATH and GSM8k
            </p>
            </div></div>
          </section>
        </section>

        <!-- 5 · Results -->
        <section id="results" class="inner">
          <header class="major"><h2>Results GSM-8K</h2></header>
          <!-- accuracy plots grid -->
          <div class="row gtr-50 gtr-uniform">
            <div class="col-6">
              <!-- *FIGURE‑PLOT‑BASE‑FORMAT* -->
              <span class="image fit"><img src="images/plot1.png" alt="Baseline format reward curve" /></span>
            </div>
            <div class="col-6">
              <!-- *FIGURE‑PLOT‑BASE‑ACC* -->
              <span class="image fit"><img src="images/plot2.png" alt="Baseline accuracy reward curve" /></span>
            </div>
            <div class="col-6">
              <!-- *FIGURE‑PLOT‑OUR‑FORMAT* -->
              <span class="image fit"><img src="images/plot3.png" alt="Our model format reward" /></span>
            </div>
            <div class="col-6">
              <!-- *FIGURE‑PLOT‑OUR‑ACC* -->
              <span class="image fit"><img src="images/plot4.png" alt="Our model accuracy reward" /></span>
            </div>
          </div>

          <!-- 5 · Results-Math -->
        <section id="results" class="inner">
          <header class="major"><h2>Results Math</h2></header>
          <!-- accuracy plots grid -->
          <div class="row gtr-50 gtr-uniform">
            <div class="col-6">
              <!-- *FIGURE‑PLOT‑BASE‑FORMAT* -->
              <span class="image fit"><img src="images/plot1m.png" alt="Baseline format reward curve" /></span>
            </div>
            <div class="col-6">
              <!-- *FIGURE‑PLOT‑BASE‑ACC* -->
              <span class="image fit"><img src="images/plot2m.png" alt="Baseline accuracy reward curve" /></span>
            </div>
            <div class="col-6">
              <!-- *FIGURE‑PLOT‑OUR‑FORMAT* -->
              <span class="image fit"><img src="images/plot3m.png" alt="Our model format reward" /></span>
            </div>
            <div class="col-6">
              <!-- *FIGURE‑PLOT‑OUR‑ACC* -->
              <span class="image fit"><img src="images/plot4m.png" alt="Our model accuracy reward" /></span>
            </div>
          </div>

          <!-- summary table -->
          <div class="table-wrapper">
            <!-- *FIGURE‑TABLE‑RESULTS* – replace with rendered HTML table / image -->
            <table>
              <thead><tr><th>Model</th><th>MATH‑500</th><th>GSM‑8K</th></tr></thead>
              <tbody>
                <tr><td>Gemma‑3‑4B‑IT (init)</td><td>10.71 %</td><td>25.57 %</td></tr>
                <tr><td>Baseline RLVR</td><td>27.32 %</td><td>78.92 %</td></tr>
                <tr><td><strong>Our full RLVR</strong></td><td><strong>33.11 %</strong></td><td><strong>81.31 %</strong></td></tr>
              </tbody>
            </table>
          </div>
        </section>

        <!-- 6 · Results -->
        <section id="results" class="style2">   <!-- keep any class name you prefer -->
          <div class="inner">
            <header class="major"><h2>Takeaways</h2></header>
        
            <p>Our experiments yield two main take-aways:</p>
            <ul>
              <li><strong>Higher sample-efficiency on the MATH dataset.</strong>
                  Both reasoning-reward variants out-learn the baseline, and the always-on check in <em>Method&nbsp;2</em> converges quickest. We do not see a similar trend for GSM-8K and we mainly believe that is the case as GSM-8K is comparatively much easier and the base model (Gemma-3-4B-IT) is already capable of solving most of the questions in it out-of-th-box.</li>
        
              <li><strong>Better generalization through stronger reasoning.</strong>
                  Encouraging logically sound chains-of-thought produces models that transfer more robustly to held-out problems, as seen in the accompanying plots and accuracy table.</li>
            </ul>
          </div>
        </section>

      <!-- 7 · Callout / future work -->
      <section id="future" class="style3">
        <div class="inner">
          <header class="major"><h2>Discussion &amp; Future Work</h2></header>
      
          <!-- updated first paragraph: removed the sentence now covered in Results -->
          <p>
            While reasoning rewards markedly improve performance, there is still room to enhance robustness and efficiency. One important direction for future work is improving out-of-distribution generalization: we want the student model to handle problems that differ from the training set. This could be achieved by curating a more diverse set of training tasks or using curricula that expose the model to a wide variety of problem types and difficulty levels. By broadening the training distribution, the model can learn more general reasoning skills and avoid overfitting to narrow patterns.<br><br>
            Another area to address is the compute cost associated with using a large teacher model for feedback. To reduce overhead, we could develop cheaper reward approximators – for example, training a smaller auxiliary model to mimic the teacher’s reasoning evaluation. Such a reward model, once trained, could quickly judge the student’s reasoning without needing to call the full teacher LLM every time. This would make the RLVR training loop more efficient. Additionally, we can explore dynamic teacher usage strategies: instead of invoking the teacher on every correct answer, the training could adaptively decide when the teacher’s input is truly needed. For instance, in early training the teacher might be used frequently to guide learning, but as the student improves, we might consult the teacher only on particularly challenging or novel problems. This selective guidance can maintain the benefits of reasoning feedback while cutting down the number of expensive teacher model calls.<br><br>
            In summary, future improvements may come from making the reward mechanism more efficient and ensuring the training data/strategy encourages broad generalization. Techniques like using distilled reward models, diversifying training examples, and smartly adjusting when to involve a teacher model are promising research directions to make reinforcement learning with LLMs both more powerful and more practical.
          </p>
        </div>
      </section>

        <!-- 8 · Team & links -->
        <section id="team" class="inner">
          <header class="major"><h2>Team &amp; Resources</h2></header>
          <p>
            Tarun Sharma, Arjun Shriyesh Chandra, Tanmay Dhadhania
          </p>
          <ul class="actions">
            <li><a href="https://github.com/arjunsinghrathore/RL_VerifiableRewards" class="button icon brands fa-github">GitHub</a></li>
            <li><a href="report.pdf" class="button">PDF Report</a></li>
            <li><a href="slides.pdf" class="button">Final Slides</a></li>
          </ul>
        </section>

      </div><!-- /main -->

      <!-- Footer -->
      <footer id="footer">
        <div class="inner">
          <ul class="icons">
            <li><a href="https://github.com/shrishriyesh" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
            <li><a href="https://www.linkedin.com/in/shriyeshchandra/" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
          </ul>
        </div>
      </footer>

    </div><!-- /wrapper -->

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
